{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting joblib==1.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/34/5b/bd0f0fb5564183884d8e35b81d06d7ec06a20d1a0c8b4c407f1554691dce/joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 303.0/303.0 KB 586.1 kB/s eta 0:00:00\n",
      "Collecting nltk==3.6.7\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c5/ea/84c7247f5c96c5a1b619fe822fb44052081ccfbe487a49d4c888306adec7/nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 757.6 kB/s eta 0:00:00\n",
      "Collecting numpy==1.19.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c7/44/e17846ef3601dcb6f118ea447439650e0c35cb4fe60274fbe24214156df2/numpy-1.19.1-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.5/14.5 MB 126.1 kB/s eta 0:00:00\n",
      "Collecting pandas==1.0.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/de/da/24e8260222f3f100cffe638189d3dffdc9ce956e9cafe60371bef258a6ce/pandas-1.0.5-cp38-cp38-manylinux1_x86_64.whl (10.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/10.0 MB 108.2 kB/s eta 0:00:00\n",
      "Collecting scikit_learn==0.23.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/47/14/c094698b7dd17cd2e289974a78d6d2df78c0d9eb0ac4d8d5fad255aaf977/scikit_learn-0.23.1-cp38-cp38-manylinux1_x86_64.whl (6.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 156.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: click in ./repo_venv/lib/python3.8/site-packages (from nltk==3.6.7->-r /home/lilounan/src/AIbro_model_repo/sentiment_model_repo/requirements.txt (line 2)) (8.0.3)\n",
      "Requirement already satisfied: tqdm in ./repo_venv/lib/python3.8/site-packages (from nltk==3.6.7->-r /home/lilounan/src/AIbro_model_repo/sentiment_model_repo/requirements.txt (line 2)) (4.60.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/54/a1/a4bca6c01eebbd641a39b861bcc18e0aeedbd205c21d809909c37f387beb/regex-2022.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 764.9/764.9 KB 190.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in ./repo_venv/lib/python3.8/site-packages (from pandas==1.0.5->-r /home/lilounan/src/AIbro_model_repo/sentiment_model_repo/requirements.txt (line 4)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./repo_venv/lib/python3.8/site-packages (from pandas==1.0.5->-r /home/lilounan/src/AIbro_model_repo/sentiment_model_repo/requirements.txt (line 4)) (2021.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in ./repo_venv/lib/python3.8/site-packages (from scikit_learn==0.23.1->-r /home/lilounan/src/AIbro_model_repo/sentiment_model_repo/requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./repo_venv/lib/python3.8/site-packages (from scikit_learn==0.23.1->-r /home/lilounan/src/AIbro_model_repo/sentiment_model_repo/requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in ./repo_venv/lib/python3.8/site-packages (from python-dateutil>=2.6.1->pandas==1.0.5->-r /home/lilounan/src/AIbro_model_repo/sentiment_model_repo/requirements.txt (line 4)) (1.15.0)\n",
      "Installing collected packages: regex, numpy, joblib, pandas, nltk, scikit_learn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.2\n",
      "    Uninstalling numpy-1.22.2:\n",
      "      Successfully uninstalled numpy-1.22.2\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.5\n",
      "    Uninstalling pandas-1.2.5:\n",
      "      Successfully uninstalled pandas-1.2.5\n",
      "  Attempting uninstall: scikit_learn\n",
      "    Found existing installation: scikit-learn 1.0.2\n",
      "    Uninstalling scikit-learn-1.0.2:\n",
      "      Successfully uninstalled scikit-learn-1.0.2\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aibro 1.1.1 requires numpy==1.19.5, but you have numpy 1.19.1 which is incompatible.\n",
      "aibro 1.1.1 requires pandas==1.2.5, but you have pandas 1.0.5 which is incompatible.\n",
      "aibro 1.1.1 requires typing-extensions==3.7.4.3, but you have typing-extensions 4.0.1 which is incompatible.\n",
      "Successfully installed joblib-1.0.0 nltk-3.6.7 numpy-1.19.1 pandas-1.0.5 regex-2022.3.2 scikit_learn-0.23.1\n",
      "Dependencies installed!\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n",
      "[nltk_data] Error loading names: <urlopen error [Errno 111] Connection\n",
      "[nltk_data]     refused>\n",
      "[nltk_data] Error loading brown: <urlopen error [Errno 111] Connection\n",
      "[nltk_data]     refused>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 111] Connection refused>\n",
      "[nltk_data] Error loading universal_tagset: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 111] Connection\n",
      "[nltk_data]     refused>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lilounan/src/AIbro_model_repo/repo_venv/lib/python3.8/site-packages/nltk/corpus/util.py\", line 84, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{zip_name}\")\n",
      "  File \"/home/lilounan/src/AIbro_model_repo/repo_venv/lib/python3.8/site-packages/nltk/data.py\", line 583, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError:\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mstopwords\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('stopwords')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/home/lilounan/nltk_data'\n",
      "    - '/home/lilounan/src/AIbro_model_repo/repo_venv/nltk_data'\n",
      "    - '/home/lilounan/src/AIbro_model_repo/repo_venv/share/nltk_data'\n",
      "    - '/home/lilounan/src/AIbro_model_repo/repo_venv/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 2, in <module>\n",
      "  File \"/home/lilounan/src/AIbro_model_repo/sentiment_model_repo/predict.py\", line 5, in <module>\n",
      "    from clean import text_cleaning  # function to clean the text\n",
      "  File \"/home/lilounan/src/AIbro_model_repo/sentiment_model_repo/clean.py\", line 36, in <module>\n",
      "    stop_words =  stopwords.words('english')\n",
      "  File \"/home/lilounan/src/AIbro_model_repo/repo_venv/lib/python3.8/site-packages/nltk/corpus/util.py\", line 121, in __getattr__\n",
      "    self.__load()\n",
      "  File \"/home/lilounan/src/AIbro_model_repo/repo_venv/lib/python3.8/site-packages/nltk/corpus/util.py\", line 86, in __load\n",
      "    raise e\n",
      "  File \"/home/lilounan/src/AIbro_model_repo/repo_venv/lib/python3.8/site-packages/nltk/corpus/util.py\", line 81, in __load\n",
      "    root = nltk.data.find(f\"{self.subdir}/{self.__name}\")\n",
      "  File \"/home/lilounan/src/AIbro_model_repo/repo_venv/lib/python3.8/site-packages/nltk/data.py\", line 583, in find\n",
      "    raise LookupError(resource_not_found)\n",
      "LookupError:\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mstopwords\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('stopwords')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/home/lilounan/nltk_data'\n",
      "    - '/home/lilounan/src/AIbro_model_repo/repo_venv/nltk_data'\n",
      "    - '/home/lilounan/src/AIbro_model_repo/repo_venv/share/nltk_data'\n",
      "    - '/home/lilounan/src/AIbro_model_repo/repo_venv/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Prediction finished without error\n",
      "\u001b[32mDRYRUN TEST: passed\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from aibro.inference import Inference\n",
    "\n",
    "# repo_path = 'pytorch_hub_DieT'\n",
    "# repo_path = 'sklearn_random_forest'\n",
    "repo_path = 'sentiment_model_repo'\n",
    "Inference.deploy(\n",
    "    repo_path,\n",
    "    dryrun=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "20005f515ab61bc93f552b80c574da19aced5c52fcaf5bd6161f151586e2ff14"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('repo_venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
